# Web Scraper Summary API

## Overview
web-scraper-summary-api is an api that allow users to automate the process of scraping webpage content and generating summaries using a large language model (LLM). The user can submit a simple POST request to specify the url and the API will scrape the content, send the content to an LLM for summarizing, save the summarized content in the database and return the summarized content to the user.

## Technical Specification
### 1. Architecture Overview
The web scraper summary api is built using the following technologies:
- [NestJS](https://docs.nestjs.com/): A powerful backend framework based on Node.js, designed for building scalable and maintainable applications.
- [MongoDB](https://www.mongodb.com/): A NoSQL database that stores data in a flexible, JSON-like format, allowing for scalable and efficient handling of unstructured or semi-structured data.
- [Puppeteer](https://www.npmjs.com/package/puppeteer): A Node library used to control a headless browser (Chrome or Chromium) for scraping webpage content.
- [AIML API](https://aimlapi.com/): An external large language model (LLM) service that generates summaries of the scraped text.
- [Prometheus](https://prometheus.io/): An open-source monitoring and alerting toolkit. It collects and stores metrics in a time-series database and provides multiple modes for visualizing data such as Grafana.

### 2. API Endpoints
- **POST /jobs**
  - **Description**: Submit a job to scrape a webpage and generate a summary.
  - **Request Body**
    ```json
        {
          "url": "https://example.com"
        }
      ```
  - **Response**
    ```json
    {
      "id": "12345",
      "url": "http://example.com",
      "status": "completed",
      "summary": "This is a short summary of the content from the URL."
    }
    ```

- **GET /jobs/:id**
  - **Description**: Retrieves the details of a specific job using its unique job ID. This route allows users to check the status of a previously submitted job, view the scraped content, or see any error messages if the job has failed.
  - **Request Parameter**:
    - **id** (path parameter): The unique identifier of the job. This ID is returned in the **POST /jobs** route.
  - **Response**:
    - **200 OK**: The job was found and the details are returned successfully.
    ```json
    {
      "id": "12345",
      "url": "http://example.com",
      "status": "failed",
      "summary": "",
      "error_message": "Failed to scrape content from the URL."
    }
    ```
    - **400 Not Found**: The job with the provided ID does not exist.
    ```json
    {
      "message": "Job not found."
    }
    ```

### 3. Workflow of the API
1. **Job Creation**: When the user sends a POST request to the `/jobs` endpoint, a new job is created with the status `pending` and the URL provided by the user. This is handled by the `JobsService`.
2. **Web Scraping**: The system uses **Puppeteer** to launch a headless browser, navigate to the specified URL, and extract the text content from the webpage.
3. **Summarization**: The **AIML API** is called to generate a summary of the scraped text.
4. **Job Update**: The job’s status, summarized content, and any error messages are stored in the database.

### 4. Database Design
| Field         | Description                                                |
|---------------|------------------------------------------------------------|
| `id`         | Unique identifier for the job (auto-generated by MongoDB). |
| `url`         | The URL of the webpage to scrape.                          |
| `status`      | The status of the job (`pending`, `completed`, `failed`).  |
| `summary`     | The summarized content (if the job is completed successfully). |
| `error_message` | Any error message (if the job failed).                   |
| `created_date` | The date when the job was created.                        |
| `updated_date` | The date when the job was last updated.                   |

## Technical Decisions
### 1. NestJS as the Backend Framework
- **Why**: NestJS is a modern, scalable, and maintainable framework built with TypeScript. It provides a powerful set of features such as dependency injection, modular architecture, and easy integration with other libraries and tools. It was chosen for its developer-friendly approach, strong TypeScript support, and the ability to easily scale the application in the future with a modular design.

### 2. Puppeteer for Web Scraping
- **Why**: Puppeteer is a Node.js library that provides a high-level API to control headless Chrome or Chromium browsers. It was selected for its ability to render and interact with web pages just like a real user, making it ideal for scraping dynamic content that requires JavaScript execution. Puppeteer’s flexibility allows for precise control over the scraping process and easy extraction of content.

### 3. AIML API for Summarizing Content
- **Why**: The AIML API is a powerful tool for generating summaries from text using large language models (LLMs). They take advantage of existing powerful LLMs and provide free tier access to their api which is suitable for proof of concept (POC) applications.

### 4. MongoDB for Storage
- **Why**: MongoDB was selected as the database due to its flexibility and scalability in handling large amounts of unstructured data. MongoDB’s schema-less nature allows for easy adjustments and growth over time without requiring strict schemas.

### 5. Prometheus for Monitoring
- **Why**: Prometheus was chosen for its ability to efficiently collect, store, and query time-series data. It is highly suited for monitoring applications in real-time, allowing us to track performance metrics such as request count, processing time, and job statuses.

## How the Solution Achieves the Admin's Desired Outcome
The Admin wanted a proof of concept (POC) of a service that allow users to generate summaries of textual content of webpages. Here's a breakdown on how the solution achieves the desired outcome:
- **Web Scraping**: The service must retrieve the text content of a webpage. We used puppeteer to scrape web pages. 
- **Summarizing Content**: The service should generate a concise summary of the web page's textual content.
- **User-Friendly API**: We exposed a simple REST API with the following routes:
  - **POST /jobs**: Accepts a URL to scrape and generate a summary.
  - **GET /jobs/:id**: Allows users to check the status, retrieve the summary and other important details.
- **Job Submission**: The service handles multiple user requests and provide status updates.
- **Storing and Fetching results**: Once the summary is generated, the service saves the summarized content in MongoDB database. This enables users to retrieve their summaries at any time by querying the job ID.

## Enhancements Needed to Make the API Production-Ready
### Below are the areas requiring additional work and suggestions:
### 1. Scalability
### Current Limitation:
- The API can handle only a limited number of concurrent scraping and summarization jobs.
- Puppeteer instances are created for each request, which can lead to resource exhaustion under heavy load.

### Proposed Solution:
- Deploy the application on a scalable infrastructure like Kubernetes or AWS ECS to handle more instance as traffic increases.
- Integrate a message queue (e.g., RabbitMQ, Redis, or AWS SQS) to manage job requests. A worker pool can process these jobs asynchronously, ensuring the API can handle high traffic without crashing.
- Use Puppeteer clusters (e.g., puppeteer-cluster) to share browser instances among multiple jobs and reduce memory usage.

### 2. Security
### Current Limitation:
- While strict validation (e.g., using class-validator) is in place, the API lacks authentication mechanism to restrict access to routes.

### Proposed Solution:
- Implement authentication mechanisms such as API keys or JWT to ensure that only authorized users can access the API.

### 3. Cost Optimization
### Current Limitation:
- Puppeteer and AIML API usage costs can escalate with increased traffic. While puppeteer is open source, puppeteer is resource intensive and running many browser instances for concurrent scraping will require more powerful servers.

### Proposed Solution:
- Implement caching mechanism (e.g., Redis) to store and reuse summaries for previously scraped URLs.
- Introduce rate limits and tiered pricing plans to balance costs and ensure fair usage.